{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770100d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3c625d",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2303.16741\n",
    "Who you Play Affects How you Play: Predicting Sports Performance using Grpah Attention with Temporal Convolution \n",
    "\n",
    "https://arxiv.org/abs/2207.13191\n",
    "GCN-WP -- Semi-Supervised Graph Convolutional Networks for Win Prediction in Esports\n",
    "\n",
    "https://www.firecrawl.dev/?twclid=2-270mk2awl7ihu5h3nss6shp14\n",
    "Turn websites into AI LLM ready data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd89fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153af5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_history = pd.read_csv(r'C:\\Users\\jcmar\\my_files\\SportsBetting\\data\\model_results_train_v2.csv')\n",
    "df_results = pd.read_csv(r'C:\\Users\\jcmar\\my_files\\SportsBetting\\data\\model_results_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbcaea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fighter_ids(df, red_col, blue_col):\n",
    "    # Combine all fighters and get unique names\n",
    "    df = df.copy()\n",
    "    all_fighters = pd.concat([df[red_col], df[blue_col]]).unique()\n",
    "    \n",
    "    # Create mapping from fighter name to unique ID\n",
    "    fighter_to_id = {fighter: idx for idx, fighter in enumerate(all_fighters)}\n",
    "    \n",
    "    # Map IDs to red and blue columns\n",
    "    df['id_red'] = df[red_col].map(fighter_to_id)\n",
    "    df['id_blue'] = df[blue_col].map(fighter_to_id)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def build_train_test(df, cols):\n",
    "    # Filter out draws and drop rows with missing values\n",
    "    df = df[df['winner'] != 2]\n",
    "    df = df[cols].dropna().copy()\n",
    "    df = df.sort_values(by='date', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    # Split index for train/test\n",
    "    train_len = int(df.shape[0] * 0.85)\n",
    "    \n",
    "    # Add split flag\n",
    "    df['split'] = ['train' if i < train_len else 'test' for i in range(df.shape[0])]\n",
    "\n",
    "    # Keep identifiers for later\n",
    "    id_cols = ['red_fighter', 'blue_fighter', 'date']\n",
    "\n",
    "    # Separate features and target\n",
    "    y = df['winner']\n",
    "\n",
    "    X = df.drop(columns=['winner'])\n",
    "\n",
    "    # Encode categorical columns\n",
    "    le = LabelEncoder()\n",
    "    if 'weight_class' in X.columns:\n",
    "        X['weight_class'] = le.fit_transform(X['weight_class'])\n",
    "\n",
    "    # Identify categorical and numerical columns (excluding id columns)\n",
    "    cat_cols = [c for c in X.select_dtypes(include=['object', 'category']).columns if c not in id_cols + ['split']]\n",
    "    numerical_cols = [c for c in X.columns if c not in cat_cols + id_cols + ['split']]\n",
    "\n",
    "    # Scale numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "    # Combine features + id columns + target + split into one DataFrame\n",
    "    combined_df = X.copy()\n",
    "    combined_df['winner'] = y.values\n",
    "\n",
    "    # Reorder columns: id_cols first, then features, then winner, then split\n",
    "    combined_df = combined_df[id_cols + [c for c in X.columns if c not in id_cols + ['split']] + ['winner', 'split']]\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def build_temporal_graph(df, red_id_col, blue_id_col, date_col,\n",
    "                         node_cols_red, node_cols_blue,\n",
    "                         split_col='split'):\n",
    "\n",
    "    df = df.sort_values(by=date_col, ascending=True).reset_index(drop=True)\n",
    "    df = build_fighter_ids(df, 'red_fighter', 'blue_fighter')\n",
    "\n",
    "    red_ids = df[red_id_col].values\n",
    "    blue_ids = df[blue_id_col].values\n",
    "    dates = pd.to_datetime(df[date_col])\n",
    "    splits = df[split_col].values  # 'train' or 'test' for each fight\n",
    "\n",
    "    # Build edge_index (flattened)\n",
    "    edge_index = torch.tensor([\n",
    "        [x for pair in zip(red_ids, blue_ids) for x in pair],  # red→blue\n",
    "        [x for pair in zip(blue_ids, red_ids) for x in pair]   # blue→red\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    # Build date_index aligned with edge_index\n",
    "    date_index = np.array([d for d in dates for _ in (0,1)])\n",
    "    split_index = np.array([s for s in splits for _ in (0,1)])  # duplicate for both directions\n",
    "    num_edges = edge_index.shape[1]\n",
    "\n",
    "    node_features_source = []\n",
    "    node_features_target = []\n",
    "    edge_features_list = []\n",
    "\n",
    "    for i in range(num_edges):\n",
    "        src_node = edge_index[0, i].item()\n",
    "        tgt_node = edge_index[1, i].item()\n",
    "        edge_date = date_index[i]\n",
    "\n",
    "        # Determine direction (red->blue or blue->red)\n",
    "        if i % 2 == 0: \n",
    "            mask = (\n",
    "                (df[red_id_col] == src_node) &\n",
    "                (df[blue_id_col] == tgt_node) &\n",
    "                (pd.to_datetime(df[date_col]) == edge_date)\n",
    "            )\n",
    "            row = df.loc[mask].iloc[0]\n",
    "            node_features_source.append(torch.tensor(row[node_cols_red].values.astype(float)))\n",
    "            node_features_target.append(torch.tensor(row[node_cols_blue].values.astype(float)))\n",
    "        else: \n",
    "            mask = (\n",
    "                (df[blue_id_col] == src_node) &\n",
    "                (df[red_id_col] == tgt_node) &\n",
    "                (pd.to_datetime(df[date_col]) == edge_date)\n",
    "            )\n",
    "            row = df.loc[mask].iloc[0]\n",
    "            node_features_source.append(torch.tensor(row[node_cols_blue].values.astype(float)))\n",
    "            node_features_target.append(torch.tensor(row[node_cols_red].values.astype(float)))\n",
    "\n",
    "        # Edge label\n",
    "        if row['winner'] == 1:\n",
    "            edge_features_list.append(torch.tensor([1 if i % 2 == 0 else 0]))\n",
    "        else:\n",
    "            edge_features_list.append(torch.tensor([0 if i % 2 == 0 else 1]))\n",
    "\n",
    "    # Stack features\n",
    "    node_features_source = torch.stack(node_features_source)\n",
    "    node_features_target = torch.stack(node_features_target)\n",
    "    edge_features_tensor = torch.stack(edge_features_list)\n",
    "\n",
    "    # Unique nodes and mapping\n",
    "    all_nodes = torch.unique(edge_index)\n",
    "    node_id_to_idx = {node.item(): i for i, node in enumerate(all_nodes)}\n",
    "    num_nodes = len(all_nodes)\n",
    "    node_feat_dim = node_features_source.shape[1]\n",
    "\n",
    "    node_features = torch.zeros((num_nodes, node_feat_dim), dtype=torch.float)\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src_idx = node_id_to_idx[edge_index[0, i].item()]\n",
    "        tgt_idx = node_id_to_idx[edge_index[1, i].item()]\n",
    "        node_features[src_idx] = node_features_source[i]\n",
    "        node_features[tgt_idx] = node_features_target[i]\n",
    "\n",
    "    edge_index_mapped = torch.tensor([\n",
    "        [node_id_to_idx[n.item()] for n in edge_index[0]],\n",
    "        [node_id_to_idx[n.item()] for n in edge_index[1]]\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    # Create train/test masks for edges\n",
    "    train_mask = torch.tensor([s == 'train' for s in split_index], dtype=torch.bool)\n",
    "    test_mask = torch.tensor([s == 'test' for s in split_index], dtype=torch.bool)\n",
    "\n",
    "    # Build PyG Data object\n",
    "    data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index_mapped,\n",
    "        \n",
    "        y=edge_features_tensor,\n",
    "        train_mask=train_mask,\n",
    "        test_mask=test_mask\n",
    "    )\n",
    "    #edge_attr=edge_features_tensor,\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb84a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = pd.read_csv(r'C:\\Users\\jcmar\\my_files\\SportsBetting\\data\\entire_odds_stats_v2.csv')\n",
    "model_cols_red = ['reach_red', 'red_age', 'height_red', 'total_bonus_red', \n",
    " 'leg_strikes_pm_red', 'body_strikes_pm_red', 'head_strikes_pm_red',\n",
    " 'clinch_strikes_pm_red', 'ground_strikes_pm_red', 'control_pr_red',\n",
    " 'kd_pr_red', 'reverse_pr_red', 'sigstrikes_absorbed_pm_red', \n",
    " 'sigstrikes_pm_red', 'sub_att_pr_red', 'red_td_accuracy_avg',\n",
    " 'red_td_defense_avg', 'red_td_landed_total', 'red_td_defended_total', \n",
    " 'ratio_td_red', 'red_sigstrike_accuracy_avg', 'red_sigstrike_defense_avg', \n",
    " 'red_kd_total', 'red_elo', 'red_glicko', 'red_glicko_rd', 'math_red', \n",
    " 'months_since_red', 'red_win_streak', 'red_lose_streak', 'win_pct_red',\n",
    " 'num_fights_red', 'num_wins_red', 'num_losses_red', 'decision_wins_red', \n",
    " 'ko_wins_red', 'sub_wins_red', 'close1_red', 'close2_red', 'open_red']\n",
    "\n",
    "model_cols_blue = ['reach_blue', 'blue_age', 'height_blue', 'total_bonus_blue', \n",
    " 'leg_strikes_pm_blue', 'body_strikes_pm_blue', 'head_strikes_pm_blue',\n",
    " 'clinch_strikes_pm_blue', 'ground_strikes_pm_blue', 'control_pr_blue',\n",
    " 'kd_pr_blue', 'reverse_pr_blue', 'sigstrikes_absorbed_pm_blue', \n",
    " 'sigstrikes_pm_blue', 'sub_att_pr_blue', 'blue_td_accuracy_avg',\n",
    " 'blue_td_defense_avg', 'blue_td_landed_total', 'blue_td_defended_total', \n",
    " 'ratio_td_blue', 'blue_sigstrike_accuracy_avg', 'blue_sigstrike_defense_avg', \n",
    " 'blue_kd_total', 'blue_elo', 'blue_glicko', 'blue_glicko_rd', 'math_blue', \n",
    " 'months_since_blue', 'blue_win_streak', 'blue_lose_streak', 'win_pct_blue',\n",
    " 'num_fights_blue', 'num_wins_blue', 'num_losses_blue', 'decision_wins_blue', \n",
    " 'ko_wins_blue', 'sub_wins_blue', 'close1_blue', 'close2_blue', 'open_blue']\n",
    "\n",
    "graph_cols = ['red_fighter','blue_fighter','date', 'winner']\n",
    "\n",
    "all_cols = model_cols_red + model_cols_blue + graph_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609d77b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red_fighter</th>\n",
       "      <th>blue_fighter</th>\n",
       "      <th>date</th>\n",
       "      <th>reach_red</th>\n",
       "      <th>red_age</th>\n",
       "      <th>height_red</th>\n",
       "      <th>total_bonus_red</th>\n",
       "      <th>leg_strikes_pm_red</th>\n",
       "      <th>body_strikes_pm_red</th>\n",
       "      <th>head_strikes_pm_red</th>\n",
       "      <th>...</th>\n",
       "      <th>num_wins_blue</th>\n",
       "      <th>num_losses_blue</th>\n",
       "      <th>decision_wins_blue</th>\n",
       "      <th>ko_wins_blue</th>\n",
       "      <th>sub_wins_blue</th>\n",
       "      <th>close1_blue</th>\n",
       "      <th>close2_blue</th>\n",
       "      <th>open_blue</th>\n",
       "      <th>winner</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sean sherk</td>\n",
       "      <td>hermes franca</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>-1.177550</td>\n",
       "      <td>2.380351</td>\n",
       "      <td>-1.170391</td>\n",
       "      <td>-0.227361</td>\n",
       "      <td>-0.568795</td>\n",
       "      <td>-1.061221</td>\n",
       "      <td>-0.759626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296671</td>\n",
       "      <td>-0.150864</td>\n",
       "      <td>-0.366751</td>\n",
       "      <td>0.331886</td>\n",
       "      <td>0.864428</td>\n",
       "      <td>0.737695</td>\n",
       "      <td>0.634364</td>\n",
       "      <td>0.803116</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anderson silva</td>\n",
       "      <td>nate marquardt</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>1.122395</td>\n",
       "      <td>2.022037</td>\n",
       "      <td>1.059753</td>\n",
       "      <td>-0.729220</td>\n",
       "      <td>-0.660576</td>\n",
       "      <td>2.428095</td>\n",
       "      <td>-0.325983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020272</td>\n",
       "      <td>-1.057681</td>\n",
       "      <td>0.658227</td>\n",
       "      <td>-0.758923</td>\n",
       "      <td>0.135477</td>\n",
       "      <td>0.352965</td>\n",
       "      <td>0.242210</td>\n",
       "      <td>0.563170</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frank mir</td>\n",
       "      <td>antoni hardonk</td>\n",
       "      <td>2007-08-25</td>\n",
       "      <td>1.582385</td>\n",
       "      <td>1.305407</td>\n",
       "      <td>1.338521</td>\n",
       "      <td>-0.729220</td>\n",
       "      <td>-0.851785</td>\n",
       "      <td>-0.339825</td>\n",
       "      <td>-1.540264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.808928</td>\n",
       "      <td>-0.604273</td>\n",
       "      <td>-0.879240</td>\n",
       "      <td>-0.213519</td>\n",
       "      <td>-0.593475</td>\n",
       "      <td>0.429911</td>\n",
       "      <td>0.320641</td>\n",
       "      <td>-0.732536</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patrick cote</td>\n",
       "      <td>kendall grove</td>\n",
       "      <td>2007-08-25</td>\n",
       "      <td>0.662406</td>\n",
       "      <td>1.126250</td>\n",
       "      <td>0.223449</td>\n",
       "      <td>-0.729220</td>\n",
       "      <td>-0.412004</td>\n",
       "      <td>-0.908303</td>\n",
       "      <td>-1.031913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.256128</td>\n",
       "      <td>-1.057681</td>\n",
       "      <td>-0.366751</td>\n",
       "      <td>-0.213519</td>\n",
       "      <td>0.135477</td>\n",
       "      <td>-1.109008</td>\n",
       "      <td>-1.247979</td>\n",
       "      <td>-1.692318</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>renato sobral</td>\n",
       "      <td>david heath</td>\n",
       "      <td>2007-08-25</td>\n",
       "      <td>0.662406</td>\n",
       "      <td>2.022037</td>\n",
       "      <td>0.780985</td>\n",
       "      <td>-0.227361</td>\n",
       "      <td>0.343272</td>\n",
       "      <td>-1.177648</td>\n",
       "      <td>-1.712630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.532528</td>\n",
       "      <td>-0.604273</td>\n",
       "      <td>-0.366751</td>\n",
       "      <td>-0.758923</td>\n",
       "      <td>0.135477</td>\n",
       "      <td>0.583803</td>\n",
       "      <td>0.477503</td>\n",
       "      <td>0.899094</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      red_fighter    blue_fighter        date  reach_red   red_age  \\\n",
       "0      sean sherk   hermes franca  2007-07-07  -1.177550  2.380351   \n",
       "1  anderson silva  nate marquardt  2007-07-07   1.122395  2.022037   \n",
       "2       frank mir  antoni hardonk  2007-08-25   1.582385  1.305407   \n",
       "3    patrick cote   kendall grove  2007-08-25   0.662406  1.126250   \n",
       "4   renato sobral     david heath  2007-08-25   0.662406  2.022037   \n",
       "\n",
       "   height_red  total_bonus_red  leg_strikes_pm_red  body_strikes_pm_red  \\\n",
       "0   -1.170391        -0.227361           -0.568795            -1.061221   \n",
       "1    1.059753        -0.729220           -0.660576             2.428095   \n",
       "2    1.338521        -0.729220           -0.851785            -0.339825   \n",
       "3    0.223449        -0.729220           -0.412004            -0.908303   \n",
       "4    0.780985        -0.227361            0.343272            -1.177648   \n",
       "\n",
       "   head_strikes_pm_red  ...  num_wins_blue  num_losses_blue  \\\n",
       "0            -0.759626  ...       0.296671        -0.150864   \n",
       "1            -0.325983  ...       0.020272        -1.057681   \n",
       "2            -1.540264  ...      -0.808928        -0.604273   \n",
       "3            -1.031913  ...      -0.256128        -1.057681   \n",
       "4            -1.712630  ...      -0.532528        -0.604273   \n",
       "\n",
       "   decision_wins_blue  ko_wins_blue  sub_wins_blue  close1_blue  close2_blue  \\\n",
       "0           -0.366751      0.331886       0.864428     0.737695     0.634364   \n",
       "1            0.658227     -0.758923       0.135477     0.352965     0.242210   \n",
       "2           -0.879240     -0.213519      -0.593475     0.429911     0.320641   \n",
       "3           -0.366751     -0.213519       0.135477    -1.109008    -1.247979   \n",
       "4           -0.366751     -0.758923       0.135477     0.583803     0.477503   \n",
       "\n",
       "   open_blue  winner  split  \n",
       "0   0.803116       1  train  \n",
       "1   0.563170       1  train  \n",
       "2  -0.732536       1  train  \n",
       "3  -1.692318       1  train  \n",
       "4   0.899094       1  train  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_graph = build_train_test(df_v2, all_cols)\n",
    "df_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d33641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_id_col = 'id_red'\n",
    "blue_id_col = 'id_blue'\n",
    "date_col = 'date'\n",
    "\n",
    "df_graph = build_train_test(df_v2, all_cols)\n",
    "data = build_temporal_graph(df_graph, red_id_col, blue_id_col, date_col, model_cols_red, model_cols_blue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865bb67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1713, 40], edge_index=[2, 10008], y=[10008, 1], train_mask=[10008], test_mask=[10008])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553acba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8506), tensor(1502))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.train_mask.sum(), data.test_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f145cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 0.6950, Test Acc 0.4920\n",
      "Epoch 5, Loss 0.6921, Test Acc 0.5193\n",
      "Epoch 10, Loss 0.6892, Test Acc 0.4987\n",
      "Epoch 15, Loss 0.6874, Test Acc 0.5047\n",
      "Epoch 20, Loss 0.6810, Test Acc 0.5087\n",
      "Epoch 25, Loss 0.6755, Test Acc 0.5120\n",
      "Epoch 30, Loss 0.6710, Test Acc 0.4927\n",
      "Epoch 35, Loss 0.6561, Test Acc 0.5346\n",
      "Epoch 40, Loss 0.6482, Test Acc 0.5439\n",
      "Epoch 45, Loss 0.6406, Test Acc 0.5473\n",
      "Epoch 50, Loss 0.6322, Test Acc 0.5426\n",
      "Epoch 55, Loss 0.6216, Test Acc 0.5559\n",
      "Epoch 60, Loss 0.6173, Test Acc 0.5399\n",
      "Epoch 65, Loss 0.6030, Test Acc 0.5679\n",
      "Epoch 70, Loss 0.5941, Test Acc 0.5706\n",
      "Epoch 75, Loss 0.5916, Test Acc 0.5712\n",
      "Epoch 80, Loss 0.5867, Test Acc 0.5772\n",
      "Epoch 85, Loss 0.5791, Test Acc 0.5732\n",
      "Epoch 90, Loss 0.5842, Test Acc 0.5726\n",
      "Epoch 95, Loss 0.5693, Test Acc 0.6119\n",
      "Epoch 100, Loss 0.5761, Test Acc 0.6012\n",
      "Epoch 105, Loss 0.5713, Test Acc 0.6012\n",
      "Epoch 110, Loss 0.5601, Test Acc 0.6072\n",
      "Epoch 115, Loss 0.5615, Test Acc 0.6059\n",
      "Epoch 120, Loss 0.5547, Test Acc 0.5972\n",
      "Epoch 125, Loss 0.5521, Test Acc 0.5899\n",
      "Epoch 130, Loss 0.5533, Test Acc 0.6079\n",
      "Epoch 135, Loss 0.5542, Test Acc 0.6065\n",
      "Epoch 140, Loss 0.5501, Test Acc 0.5726\n",
      "Epoch 145, Loss 0.5458, Test Acc 0.5786\n",
      "Epoch 150, Loss 0.5480, Test Acc 0.5905\n",
      "Epoch 155, Loss 0.5477, Test Acc 0.5925\n",
      "Epoch 160, Loss 0.5497, Test Acc 0.5872\n",
      "Epoch 165, Loss 0.5434, Test Acc 0.6072\n",
      "Epoch 170, Loss 0.5444, Test Acc 0.6012\n",
      "Epoch 175, Loss 0.5428, Test Acc 0.6192\n",
      "Epoch 180, Loss 0.5478, Test Acc 0.6238\n",
      "Epoch 185, Loss 0.5333, Test Acc 0.6052\n",
      "Epoch 190, Loss 0.5297, Test Acc 0.6238\n",
      "Epoch 195, Loss 0.5342, Test Acc 0.6092\n",
      "Epoch 200, Loss 0.5346, Test Acc 0.6065\n",
      "Epoch 205, Loss 0.5359, Test Acc 0.6338\n",
      "Epoch 210, Loss 0.5292, Test Acc 0.6252\n",
      "Epoch 215, Loss 0.5351, Test Acc 0.5925\n",
      "Epoch 220, Loss 0.5354, Test Acc 0.6059\n",
      "Epoch 225, Loss 0.5295, Test Acc 0.6125\n",
      "Epoch 230, Loss 0.5304, Test Acc 0.6132\n",
      "Epoch 235, Loss 0.5257, Test Acc 0.6025\n",
      "Epoch 240, Loss 0.5279, Test Acc 0.6218\n",
      "Epoch 245, Loss 0.5231, Test Acc 0.6145\n",
      "Epoch 250, Loss 0.5265, Test Acc 0.6125\n",
      "Epoch 255, Loss 0.5196, Test Acc 0.6198\n",
      "Epoch 260, Loss 0.5212, Test Acc 0.5912\n",
      "Epoch 265, Loss 0.5235, Test Acc 0.6032\n",
      "Epoch 270, Loss 0.5247, Test Acc 0.6212\n",
      "Epoch 275, Loss 0.5200, Test Acc 0.6119\n",
      "Epoch 280, Loss 0.5133, Test Acc 0.6138\n",
      "Epoch 285, Loss 0.5119, Test Acc 0.5972\n",
      "Epoch 290, Loss 0.5193, Test Acc 0.6005\n",
      "Epoch 295, Loss 0.5131, Test Acc 0.6205\n",
      "Epoch 300, Loss 0.5196, Test Acc 0.6039\n",
      "Epoch 305, Loss 0.5191, Test Acc 0.6218\n",
      "Epoch 310, Loss 0.5109, Test Acc 0.6391\n",
      "Epoch 315, Loss 0.5135, Test Acc 0.6059\n",
      "Epoch 320, Loss 0.5092, Test Acc 0.6372\n",
      "Epoch 325, Loss 0.5185, Test Acc 0.6345\n",
      "Epoch 330, Loss 0.5085, Test Acc 0.6099\n",
      "Epoch 335, Loss 0.5204, Test Acc 0.6052\n",
      "Epoch 340, Loss 0.5087, Test Acc 0.6145\n",
      "Epoch 345, Loss 0.5116, Test Acc 0.6232\n",
      "Epoch 350, Loss 0.5106, Test Acc 0.6172\n",
      "Epoch 355, Loss 0.5096, Test Acc 0.6072\n",
      "Epoch 360, Loss 0.5044, Test Acc 0.6085\n",
      "Epoch 365, Loss 0.5060, Test Acc 0.5992\n",
      "Epoch 370, Loss 0.5117, Test Acc 0.6252\n",
      "Epoch 375, Loss 0.5124, Test Acc 0.5992\n",
      "Epoch 380, Loss 0.5076, Test Acc 0.6085\n",
      "Epoch 385, Loss 0.5120, Test Acc 0.6132\n",
      "Epoch 390, Loss 0.5052, Test Acc 0.6285\n",
      "Epoch 395, Loss 0.5027, Test Acc 0.6272\n",
      "Epoch 400, Loss 0.5017, Test Acc 0.6372\n",
      "Epoch 405, Loss 0.5012, Test Acc 0.6205\n",
      "Epoch 410, Loss 0.5029, Test Acc 0.6052\n",
      "Epoch 415, Loss 0.5053, Test Acc 0.6045\n",
      "Epoch 420, Loss 0.5083, Test Acc 0.6265\n",
      "Epoch 425, Loss 0.4991, Test Acc 0.6125\n",
      "Epoch 430, Loss 0.5039, Test Acc 0.6019\n",
      "Epoch 435, Loss 0.5032, Test Acc 0.6138\n",
      "Epoch 440, Loss 0.5026, Test Acc 0.6325\n",
      "Epoch 445, Loss 0.5069, Test Acc 0.6278\n",
      "Epoch 450, Loss 0.5030, Test Acc 0.6065\n",
      "Epoch 455, Loss 0.5009, Test Acc 0.6258\n",
      "Epoch 460, Loss 0.4999, Test Acc 0.6172\n",
      "Epoch 465, Loss 0.4957, Test Acc 0.5899\n",
      "Epoch 470, Loss 0.4962, Test Acc 0.6332\n",
      "Epoch 475, Loss 0.4969, Test Acc 0.6238\n",
      "Epoch 480, Loss 0.5010, Test Acc 0.6158\n",
      "Epoch 485, Loss 0.4887, Test Acc 0.6012\n",
      "Epoch 490, Loss 0.4985, Test Acc 0.6198\n",
      "Epoch 495, Loss 0.4927, Test Acc 0.6065\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # GAT layers\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=2, concat=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels * 2)\n",
    "        \n",
    "        self.conv2 = GATConv(hidden_channels*2, hidden_channels, heads=1, concat=True)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels, heads=1, concat=True)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Edge MLP\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4*hidden_channels, hidden_channels*2),  # node embeddings + graph pooled\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            torch.nn.Linear(hidden_channels*2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "            torch.nn.Linear(hidden_channels, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Node embeddings through 3-layer GAT\n",
    "        x1 = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x1 = self.dropout(x1)\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1, edge_index)))\n",
    "        x2 = self.dropout(x2)\n",
    "        x = F.relu(self.bn3(self.conv3(x2, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
    "        graph_feat = global_mean_pool(x, batch)      # [1, hidden_channels]\n",
    "        graph_feat_expanded = graph_feat[batch]      # [num_nodes, hidden_channel\n",
    "        # Graph-level pooling\n",
    "        graph_feat = global_mean_pool(x, batch)  # [num_graphs, hidden_channels]\n",
    "        # Map graph-level feature to nodes\n",
    "        graph_feat_expanded = graph_feat[batch]  # repeat per node\n",
    "\n",
    "        # Gather embeddings for source and target of each edge\n",
    "        src, dst = edge_index\n",
    "        edge_feat = torch.cat([\n",
    "            x[src],                # [num_edges, hidden_channels]\n",
    "            x[dst],                # [num_edges, hidden_channels]\n",
    "            graph_feat_expanded[src],  # [num_edges, hidden_channels]\n",
    "            graph_feat_expanded[dst]   # [num_edges, hidden_channels]\n",
    "        ], dim=1)  # total: 4 * hidden_channels\n",
    "\n",
    "        # Predict edge outcomes\n",
    "        return self.edge_mlp(edge_feat)\n",
    "\n",
    "# ----------------------------\n",
    "node_feat_dim = len(model_cols_red)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EdgeClassifier(node_feat_dim, hidden_channels=32).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "   \n",
    "    # Forward pass\n",
    "    out = model(data.x, data.edge_index, data.batch)  # no edge_attr needed\n",
    "\n",
    "    # Compute loss only on train edges\n",
    "    train_out = out[data.train_mask]\n",
    "    train_y = data.y[data.train_mask].view(-1).long()  # cross_entropy expects long\n",
    "    loss = F.cross_entropy(train_out, train_y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on test edges\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_out = out[data.test_mask]\n",
    "        test_y = data.y[data.test_mask].view(-1).long()\n",
    "        pred = test_out.argmax(dim=1)\n",
    "        acc = (pred == test_y).float().mean().item()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss {loss.item():.4f}, Test Acc {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce86a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
