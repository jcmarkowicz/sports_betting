{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b07216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import defaultdict\n",
    " \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05258c",
   "metadata": {},
   "source": [
    "https://ojs.aaai.org/index.php/AIIDE/article/view/5233/5089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b3cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v2 = pd.read_csv(r'C:\\Users\\jcmar\\my_files\\SportsBetting\\data\\entire_odds_stats_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9d8eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574176514961026 1.1071324407147025 0.6589784350258932\n"
     ]
    }
   ],
   "source": [
    "def elo_rating(df,k, w90=400):\n",
    "    \n",
    "    elo_dic = defaultdict(list)\n",
    "    red_elo = []\n",
    "    blue_elo = []\n",
    "    mu_red_col = []\n",
    "    mu_blue_col = []\n",
    "    for _, row in df.iterrows(): \n",
    "        red_name = row['red_fighter']\n",
    "        blue_name = row['blue_fighter']\n",
    "\n",
    "        if red_name not in elo_dic:\n",
    "            elo_dic[red_name] = [1500]\n",
    "        \n",
    "        if blue_name not in elo_dic:\n",
    "            elo_dic[blue_name] = [1500]\n",
    "\n",
    "        prev_blue = elo_dic[blue_name][-1] #elo pre fight\n",
    "        prev_red = elo_dic[red_name][-1]\n",
    "        red_elo.append(prev_red)\n",
    "        blue_elo.append(prev_blue)\n",
    "        \n",
    "        if row['winner'] == 1 and pd.notna(row['winner']):\n",
    "\n",
    "            d = prev_red - prev_blue\n",
    "            mu_red = 1 / (1 + 10**(-d/w90))\n",
    "            red_new = prev_red + k * (1-mu_red) #red wins\n",
    "\n",
    "            d = prev_blue - prev_red\n",
    "            mu_blue = 1 / (1 + 10**(-d/w90))\n",
    "            blue_new = prev_blue + k * (0-mu_blue) #blue loses\n",
    "            \n",
    "            elo_dic[red_name].append(red_new)\n",
    "            elo_dic[blue_name].append(blue_new)\n",
    "\n",
    "        if row['winner'] == 0 and pd.notna(row['winner']):\n",
    "\n",
    "            d = prev_blue - prev_red\n",
    "            mu_blue = 1 / (1 + 10**(-d/w90))\n",
    "            blue_new = prev_blue + k * (1-mu_blue) #blue wins\n",
    "\n",
    "            d = prev_red - prev_blue\n",
    "            mu_red = 1 / (1 + 10**(-d/w90))\n",
    "            red_new = prev_red + k * (0-mu_red) #red loses\n",
    "            \n",
    "            elo_dic[red_name].append(red_new)\n",
    "            elo_dic[blue_name].append(blue_new)\n",
    "        \n",
    "        mu_red_col.append(mu_red)\n",
    "        mu_blue_col.append(mu_blue)\n",
    "\n",
    "    return np.column_stack([red_elo, blue_elo, mu_red_col, mu_blue_col])\n",
    "\n",
    "elo_red_blue = elo_rating(df_v2, 45, 100)\n",
    "df_elo = pd.DataFrame({'elo_red':elo_red_blue[:,0], 'elo_blue':elo_red_blue[:,1],\n",
    "                       'mu_red':elo_red_blue[:,2], 'mu_blue':elo_red_blue[:,3], 'winner':df_v2['winner']})\n",
    "\n",
    "df_elo['pred'] = np.where(df_elo['elo_red']>=df_elo['elo_blue'],1,0)\n",
    "df_elo['pred_mu'] = np.where(df_elo['mu_red']>=df_elo['mu_blue'],df_elo['mu_red'],df_elo['mu_blue'])\n",
    "df_elo['opp_mu'] = np.where(df_elo['mu_red']<=df_elo['mu_blue'],df_elo['mu_red'],df_elo['mu_blue'])\n",
    "\n",
    "total_correct = np.sum(np.where(df_elo['pred'] == df_elo['winner'], 1,0))\n",
    "accuracy = total_correct/df_elo.shape[0]\n",
    "calibration = df_elo['pred_mu'].sum() / total_correct\n",
    "\n",
    "y_true = df_elo['winner'].values  \n",
    "y_pred = df_elo['pred_mu'].values  \n",
    "\n",
    "log_loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / df_elo.shape[0]\n",
    "df_elo['log_loss'] = log_loss\n",
    "print(accuracy, calibration, np.sum(log_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c0939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bddd1fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elo_red</th>\n",
       "      <th>elo_blue</th>\n",
       "      <th>mu_red</th>\n",
       "      <th>mu_blue</th>\n",
       "      <th>winner</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7949</th>\n",
       "      <td>1557.619656</td>\n",
       "      <td>1537.964183</td>\n",
       "      <td>0.611253</td>\n",
       "      <td>0.388747</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7950</th>\n",
       "      <td>1524.448753</td>\n",
       "      <td>1536.266677</td>\n",
       "      <td>0.432387</td>\n",
       "      <td>0.567613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>1565.574111</td>\n",
       "      <td>1549.817127</td>\n",
       "      <td>0.589722</td>\n",
       "      <td>0.410278</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>1576.795143</td>\n",
       "      <td>1546.669855</td>\n",
       "      <td>0.666781</td>\n",
       "      <td>0.333219</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>1518.380750</td>\n",
       "      <td>1548.390970</td>\n",
       "      <td>0.333808</td>\n",
       "      <td>0.666192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          elo_red     elo_blue    mu_red   mu_blue  winner  pred\n",
       "7949  1557.619656  1537.964183  0.611253  0.388747       1     1\n",
       "7950  1524.448753  1536.266677  0.432387  0.567613       1     0\n",
       "7951  1565.574111  1549.817127  0.589722  0.410278       0     1\n",
       "7952  1576.795143  1546.669855  0.666781  0.333219       1     1\n",
       "7953  1518.380750  1548.390970  0.333808  0.666192       0     0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_elo.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from math import log10\n",
    "\n",
    "# ----------------------------\n",
    "# Helper: expected probability\n",
    "# ----------------------------\n",
    "def expected_prob(r_a, r_b, scale=400.0):\n",
    "    # p(A wins) = 1 / (1 + 10^((Rb - Ra)/scale))\n",
    "    return 1.0 / (1.0 + 10.0 ** ((r_b - r_a) / scale))\n",
    "\n",
    "# ----------------------------\n",
    "# MOV scaling functions\n",
    "# ----------------------------\n",
    "def mov_linear(w):\n",
    "    return max(w - 1.0, 1.0)\n",
    "\n",
    "def mov_log(w):\n",
    "    # avoid log(0)\n",
    "    return np.log(150.0 * max(w - 1.0, 0.0) + 1.0)\n",
    "\n",
    "def mov_sqrt(w):\n",
    "    return np.sqrt(100.0 * max(w, 0.0))\n",
    "\n",
    "def mov_exp(w):\n",
    "    return 3.0 ** max(w, 0.0)\n",
    "\n",
    "MOV_MAP = {\n",
    "    \"linear\": mov_linear,\n",
    "    \"log\": mov_log,\n",
    "    \"sqrt\": mov_sqrt,\n",
    "    \"exp\": mov_exp\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Single Elo run function\n",
    "# ----------------------------\n",
    "def run_elo_on_matches(matches_df,\n",
    "                       base_k=20.0,\n",
    "                       mov_mode=\"log\",\n",
    "                       cutoff_rating=None,\n",
    "                       cutoff_k_scale=0.5,\n",
    "                       w90=None,\n",
    "                       regress_to_mean=0.0,\n",
    "                       regress_every_n_matches=None,\n",
    "                       verbose=False,\n",
    "                       predict_on=None,\n",
    "                       scale_override=None):\n",
    "    \"\"\"\n",
    "    Run Elo through matches (chronological) and optionally predict on a holdout set.\n",
    "\n",
    "    matches_df must include columns:\n",
    "      - 'date' (chronological order assumed)\n",
    "      - 'player_a', 'player_b' (ids)\n",
    "      - 'score_a', 'score_b' (numeric, for margin)\n",
    "      - 'outcome_a' (1 if A wins, else 0)  OR we compute from score\n",
    "\n",
    "    Parameters:\n",
    "      - base_k: base K factor\n",
    "      - mov_mode: 'linear'|'log'|'sqrt'|'exp'\n",
    "      - cutoff_rating: rating threshold above which K is scaled\n",
    "      - cutoff_k_scale: multiplier when above cutoff\n",
    "      - w90: rating difference corresponding to 90% win prob -> used to compute scale if provided\n",
    "      - regress_to_mean: fraction to regress ratings toward global mean (0=no regression)\n",
    "      - regress_every_n_matches: if not None, apply regression every N processed matches\n",
    "      - predict_on: optional DataFrame of matches to collect predictions for (same schema)\n",
    "      - scale_override: numeric scale used in expected_prob; if None and w90 provided, compute from w90; else default 400\n",
    "    Returns:\n",
    "      - preds (list of predicted probs for rows in predict_on in same order) if predict_on given\n",
    "      - final_ratings dict\n",
    "    \"\"\"\n",
    "    # copy to avoid editing\n",
    "    df = matches_df.copy().reset_index(drop=True)\n",
    "\n",
    "    # compute scale parameter\n",
    "    if scale_override is not None:\n",
    "        scale = float(scale_override)\n",
    "    elif w90 is not None:\n",
    "        p = 0.9\n",
    "        denom = log10(p / (1 - p))  # ~= log10(9) ~= 0.9542\n",
    "        scale = float(w90) / denom\n",
    "    else:\n",
    "        scale = 400.0\n",
    "\n",
    "    mov_func = MOV_MAP.get(mov_mode)\n",
    "    if mov_func is None:\n",
    "        raise ValueError(\"mov_mode must be one of \" + \", \".join(MOV_MAP.keys()))\n",
    "\n",
    "    # ratings store\n",
    "    ratings = {}\n",
    "    default_rating = 1500.0\n",
    "    # optional K per player (kept simple here as constant base_k, but could be per-player)\n",
    "    # processed count for regression schedule\n",
    "    processed = 0\n",
    "\n",
    "    # helper to get rating\n",
    "    def get_rating(player):\n",
    "        return ratings.get(player, default_rating)\n",
    "\n",
    "    # predictions storage if asked\n",
    "    preds = []\n",
    "    pred_ids = []\n",
    "\n",
    "    # iterate chronologically\n",
    "    for idx, row in df.iterrows():\n",
    "        a = row['player_a']\n",
    "        b = row['player_b']\n",
    "        sa = row.get('outcome_a', None)\n",
    "        if sa is None:\n",
    "            sa = 1 if row['score_a'] > row['score_b'] else 0\n",
    "        sb = 1 - sa\n",
    "\n",
    "        ra = get_rating(a)\n",
    "        rb = get_rating(b)\n",
    "\n",
    "        # expected prob using scale\n",
    "        pa = expected_prob(ra, rb, scale=scale)\n",
    "\n",
    "        # margin of victory\n",
    "        w = abs(row['score_a'] - row['score_b'])\n",
    "        # ensure w>0 for some functions\n",
    "        if w <= 0:\n",
    "            w = 1.0\n",
    "\n",
    "        k_mult = mov_func(w)\n",
    "\n",
    "        # apply cutoff scaling if rating above threshold (apply if either or both > cutoff)\n",
    "        effective_k = base_k * k_mult\n",
    "        if cutoff_rating is not None:\n",
    "            # if both above cutoff, scale down (you can change this rule)\n",
    "            if ra >= cutoff_rating and rb >= cutoff_rating:\n",
    "                effective_k *= cutoff_k_scale\n",
    "\n",
    "        # update ratings\n",
    "        delta = effective_k * (sa - pa)\n",
    "        ratings[a] = ra + delta\n",
    "        ratings[b] = rb - delta\n",
    "\n",
    "        processed += 1\n",
    "        # optional regression to mean periodically\n",
    "        if regress_to_mean and regress_every_n_matches and (processed % regress_every_n_matches == 0):\n",
    "            # regress all ratings toward mean rating\n",
    "            if len(ratings) > 0:\n",
    "                mean_rating = np.mean(list(ratings.values()))\n",
    "                for k in list(ratings.keys()):\n",
    "                    ratings[k] = ratings[k] + regress_to_mean * (default_rating - ratings[k])\n",
    "\n",
    "    # If prediction required on a separate DataFrame (e.g., validation set), compute probs using final ratings\n",
    "    if predict_on is not None:\n",
    "        preds = []\n",
    "        for _, row in predict_on.reset_index(drop=True).iterrows():\n",
    "            a = row['player_a']; b = row['player_b']\n",
    "            ra = ratings.get(a, default_rating)\n",
    "            rb = ratings.get(b, default_rating)\n",
    "            p = expected_prob(ra, rb, scale=scale)\n",
    "            preds.append(p)\n",
    "        return np.array(preds), ratings\n",
    "\n",
    "    return None, ratings\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cross-validation grid search (expanding window)\n",
    "# ------------------------------------------------------------\n",
    "def scope_grid_search(matches_df,\n",
    "                      param_grid,\n",
    "                      n_splits=5,\n",
    "                      metric=\"logloss\",\n",
    "                      initial_train_frac=0.2,\n",
    "                      val_window_frac=0.1,\n",
    "                      verbose=True):\n",
    "    \"\"\"\n",
    "    Grid search for SCOPE-style Elo parameters using expanding time-based CV.\n",
    "\n",
    "    matches_df:\n",
    "      chronological DataFrame with columns: 'date' (or already chronological index), 'player_a','player_b','score_a','score_b'\n",
    "    param_grid: dict of lists, e.g.\n",
    "       {\n",
    "         \"base_k\": [10, 20],\n",
    "         \"mov_mode\": [\"log\", \"sqrt\"],\n",
    "         \"cutoff_rating\": [None, 1800],\n",
    "         \"cutoff_k_scale\": [0.5, 1.0],\n",
    "         \"w90\": [None, 400],\n",
    "         \"regress_to_mean\": [0.0, 0.1],\n",
    "         \"regress_every_n_matches\": [None, 1000]\n",
    "       }\n",
    "    n_splits: how many expanding folds\n",
    "    metric: 'logloss' or 'brier'\n",
    "    Returns:\n",
    "      DataFrame with param combo and mean CV metric (lower better). Sorted ascending.\n",
    "    \"\"\"\n",
    "\n",
    "    df = matches_df.copy().reset_index(drop=True)\n",
    "    N = len(df)\n",
    "    train_start = 0\n",
    "    results = []\n",
    "\n",
    "    # build list of candidate param tuples\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = list(product(*[param_grid[k] for k in keys]))\n",
    "    total = len(combos)\n",
    "    if verbose:\n",
    "        print(f\"Running {total} parameter combinations\")\n",
    "\n",
    "    # prepare split boundaries\n",
    "    init_train = int(N * initial_train_frac)\n",
    "    val_window = int(N * val_window_frac)\n",
    "    if init_train < 10 or val_window < 1:\n",
    "        raise ValueError(\"initial_train_frac or val_window_frac too small for dataset size\")\n",
    "\n",
    "    # create split start indices (expanding)\n",
    "    starts = []\n",
    "    step = (N - init_train - val_window) // max(1, (n_splits - 1))\n",
    "    for i in range(n_splits):\n",
    "        tr_end = init_train + i * step\n",
    "        val_start = tr_end\n",
    "        val_end = min(val_start + val_window, N)\n",
    "        starts.append((0, tr_end, val_start, val_end))\n",
    "\n",
    "    for combo_idx, combo in enumerate(combos, 1):\n",
    "        params = dict(zip(keys, combo))\n",
    "        cv_scores = []\n",
    "\n",
    "        for (tr0, tr_end, val_start, val_end) in starts:\n",
    "            if val_end <= val_start or tr_end <= tr0:\n",
    "                continue\n",
    "            train_df = df.iloc[tr0:tr_end].reset_index(drop=True)\n",
    "            val_df = df.iloc[val_start:val_end].reset_index(drop=True)\n",
    "\n",
    "            # Run Elo on training matches\n",
    "            _, ratings = run_elo_on_matches(train_df,\n",
    "                                            base_k=params.get(\"base_k\", 20.0),\n",
    "                                            mov_mode=params.get(\"mov_mode\", \"log\"),\n",
    "                                            cutoff_rating=params.get(\"cutoff_rating\", None),\n",
    "                                            cutoff_k_scale=params.get(\"cutoff_k_scale\", 1.0),\n",
    "                                            w90=params.get(\"w90\", None),\n",
    "                                            regress_to_mean=params.get(\"regress_to_mean\", 0.0),\n",
    "                                            regress_every_n_matches=params.get(\"regress_every_n_matches\", None),\n",
    "                                            verbose=False)\n",
    "\n",
    "            # Predict probs for validation using learned ratings\n",
    "            preds = []\n",
    "            y_true = []\n",
    "            # compute expected with scale derived from w90 or default 400\n",
    "            if params.get(\"w90\") is not None:\n",
    "                p = 0.9\n",
    "                denom = log10(p / (1 - p))\n",
    "                scale = float(params.get(\"w90\")) / denom\n",
    "            else:\n",
    "                scale = 400.0\n",
    "\n",
    "            for _, row in val_df.iterrows():\n",
    "                a = row['player_a']; b = row['player_b']\n",
    "                ra = ratings.get(a, 1500.0)\n",
    "                rb = ratings.get(b, 1500.0)\n",
    "                preds.append(expected_prob(ra, rb, scale=scale))\n",
    "                y_true.append(1 if row.get('outcome_a', None) == 1 else (1 if row['score_a'] > row['score_b'] else 0))\n",
    "\n",
    "            preds = np.clip(np.array(preds), 1e-12, 1 - 1e-12)\n",
    "            y_true = np.array(y_true)\n",
    "\n",
    "            if metric == \"logloss\":\n",
    "                sc = log_loss(y_true, preds)\n",
    "            elif metric == \"brier\":\n",
    "                sc = np.mean((preds - y_true) ** 2)\n",
    "            else:\n",
    "                raise ValueError(\"metric must be 'logloss' or 'brier'\")\n",
    "\n",
    "            cv_scores.append(sc)\n",
    "\n",
    "        if len(cv_scores) == 0:\n",
    "            mean_score = np.nan\n",
    "        else:\n",
    "            mean_score = float(np.mean(cv_scores))\n",
    "\n",
    "        row = params.copy()\n",
    "        row['cv_score'] = mean_score\n",
    "        results.append(row)\n",
    "\n",
    "        if verbose and combo_idx % max(1, total // 10) == 0:\n",
    "            print(f\"Combo {combo_idx}/{total} done, cv_score={mean_score:.4f}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('cv_score').reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage:\n",
    "# ---------------------------\n",
    "# matches_df needs columns: date/player_a/player_b/score_a/score_b  (chronological order)\n",
    "# param_grid = {\n",
    "#   \"base_k\": [10, 20, 40],\n",
    "#   \"mov_mode\": [\"linear\",\"log\",\"sqrt\"],\n",
    "#   \"cutoff_rating\":[None, 1800],\n",
    "#   \"cutoff_k_scale\":[0.5, 1.0],\n",
    "#   \"w90\":[None, 300, 500],\n",
    "#   \"regress_to_mean\":[0.0, 0.05],\n",
    "#   \"regress_every_n_matches\":[None, 1000]\n",
    "# }\n",
    "# results_df = scope_grid_search(matches_df, param_grid, n_splits=5, metric=\"logloss\")\n",
    "# print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
